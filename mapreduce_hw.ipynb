{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37f3710d-80d1-4956-a8cf-9da7b61732eb",
   "metadata": {},
   "source": [
    "# Mapreduce assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eef0b03-e9a1-4ba5-9d15-90b52fe91402",
   "metadata": {},
   "source": [
    "## Imports and set dirs/vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d84430b6-c700-4144-9a08-2998cea741d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from spellchecker import SpellChecker\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "111080a3-f5e4-4e82-b62b-8e40552cc47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "\n",
    "WD = \"/Users/drmariamisayan/mapreducer_assignment\"\n",
    "\n",
    "# Create if doesn't exist\n",
    "os.makedirs(WD, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d698c58-79f5-462a-8fa2-8f7448373a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change directory to WD \n",
    "os.chdir(WD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "118fe7bf-4da9-4061-b159-1e6852ab24e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mfile1.txt\u001b[m\u001b[m\n",
      "\u001b[31mfile2.txt\u001b[m\u001b[m\n",
      "\u001b[31mmapper1.py\u001b[m\u001b[m\n",
      "\u001b[31mmapper2.py\u001b[m\u001b[m\n",
      "\u001b[30m\u001b[43moutput1\u001b[m\u001b[m\n",
      "\u001b[30m\u001b[43moutput2\u001b[m\u001b[m\n",
      "\u001b[31mreducer1.py\u001b[m\u001b[m\n",
      "\u001b[31mreducer2.py\u001b[m\u001b[m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check contents\n",
    "os.system(\"ls -1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2469e5cc-c1b2-4e04-aa8f-8d57065095ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check contents of files\n",
    "os.system(\"head **/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5c0f98c4-542d-483a-9416-4d75616c646a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for outputs\n",
    "for num in (1,2):\n",
    "    os.makedirs(f\"{WD}/output{num}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52a970ec-b271-448e-8162-e3df4047e424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36minput1\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36minput2\u001b[m\u001b[m\n",
      "\u001b[31mmapper1.py\u001b[m\u001b[m\n",
      "\u001b[31mmapper2.py\u001b[m\u001b[m\n",
      "\u001b[30m\u001b[43moutput1\u001b[m\u001b[m\n",
      "\u001b[30m\u001b[43moutput2\u001b[m\u001b[m\n",
      "\u001b[31mreducer1.py\u001b[m\u001b[m\n",
      "\u001b[31mreducer2.py\u001b[m\u001b[m\n",
      "\n",
      "./input1:\n",
      "\u001b[31mfile1.txt\u001b[m\u001b[m\n",
      "\n",
      "./input2:\n",
      "\u001b[31mfile2.txt\u001b[m\u001b[m\n",
      "\n",
      "./output1:\n",
      "\n",
      "./output2:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check contents and new dirs\n",
    "os.system(\"ls -1R\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fc456a-029e-4f56-bfee-6cd73ec4ba81",
   "metadata": {},
   "source": [
    "## Start HDFS and copy files over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "640c450e-6c50-4608-9d28-35808732ba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set path variables\n",
    "# Set user\n",
    "user = os.environ[\"USER\"]\n",
    "\n",
    "# Set Hadoop home\n",
    "os.environ[\"HADOOP_HOME\"] = \"/usr/local/Cellar/hadoop/3.4.1/libexec\"\n",
    "\n",
    "# Add sbin to PATH\n",
    "os.environ[\"PATH\"] = os.path.join(os.environ[\"HADOOP_HOME\"], \"sbin\") + os.pathsep + os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57046cf8-2b9c-4c7a-bf66-6b550ba2478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"start-all.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f215d00d-b019-41d4-b8c7-008abf63bb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for outputs\n",
    "for num in (1,2):\n",
    "    os.system(f\"hdfs dfs -mkdir -p /user/{user}/input{num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5cbe2759-4468-4558-90f4-735bf8e5d0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 22:59:12,630 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2025-09-21 22:59:16,154 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Copy files to new input dir\n",
    "for num in (1,2):\n",
    "    os.system(f\"hdfs dfs -put ./input{num}/file{num}.txt /user/{user}/input{num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1310753d-b966-428f-ade7-049cd43304d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 22:59:19,583 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x   - drmariamisayan supergroup          0 2025-09-21 22:55 /user/drmariamisayan/input\n",
      "-rw-r--r--   1 drmariamisayan supergroup      22335 2025-09-21 22:55 /user/drmariamisayan/input/file1.txt\n",
      "-rw-r--r--   1 drmariamisayan supergroup      20417 2025-09-21 22:55 /user/drmariamisayan/input/file2.txt\n",
      "drwxr-xr-x   - drmariamisayan supergroup          0 2025-09-21 22:59 /user/drmariamisayan/input1\n",
      "-rw-r--r--   1 drmariamisayan supergroup      22335 2025-09-21 22:59 /user/drmariamisayan/input1/file1.txt\n",
      "drwxr-xr-x   - drmariamisayan supergroup          0 2025-09-21 22:59 /user/drmariamisayan/input2\n",
      "-rw-r--r--   1 drmariamisayan supergroup      20417 2025-09-21 22:59 /user/drmariamisayan/input2/file2.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check contents\n",
    "os.system(f\"hdfs dfs -ls -R /user/{user}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9fdfe5-4ec4-46b2-b075-f65a69b0cdf3",
   "metadata": {},
   "source": [
    "## Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2d9561-7ad0-4d31-a8ce-80bc732d678f",
   "metadata": {},
   "source": [
    "### Check scripts for file 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a29a65b-82e8-4670-b365-bb380134efb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python3\n",
      "import sys, re\n",
      "\n",
      "for line in sys.stdin:\n",
      "    for word in line.strip().split():\n",
      "        # keep only letters and apostrophes\n",
      "        word = re.sub(r\"[^a-zA-Z']+\", \"\", word).lower()\n",
      "        if word:\n",
      "            print(f\"{word}\\t1\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(f\"cat mapper1.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a8ed67d-61f6-4ff4-bdea-fda1c778e84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python3\n",
      "import sys\n",
      "\n",
      "counts = {}\n",
      "\n",
      "for line in sys.stdin:\n",
      "    try:\n",
      "        word, n = line.strip().split('\\t')\n",
      "        counts[word] = counts.get(word, 0) + int(n)\n",
      "    except:\n",
      "        continue\n",
      "\n",
      "for word, total in counts.items():\n",
      "    print(f\"{word}\\t{total}\")\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(f\"cat reducer1.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9528a88-da22-405b-938b-be9fe04d67cc",
   "metadata": {},
   "source": [
    "### Check scripts for file 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a2c6036-b734-4560-82e7-025afcc14b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python3\n",
      "import sys, re\n",
      "from spellchecker import SpellChecker\n",
      "\n",
      "spell = SpellChecker()\n",
      "\n",
      "for line in sys.stdin:\n",
      "    words = re.findall(r\"[a-zA-Z]+\", line.lower())\n",
      "    for w in words:\n",
      "        if w not in spell:   # non-English\n",
      "            print(f\"{w}\\t1\")\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(f\"cat mapper2.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f4d2331-03f3-4c25-8413-26020ea14593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python3\n",
      "import sys\n",
      "from collections import defaultdict\n",
      "\n",
      "counts = defaultdict(int)\n",
      "for line in sys.stdin:\n",
      "    word, n = line.strip().split('\\t')\n",
      "    counts[word] += int(n)\n",
      "\n",
      "for word, total in counts.items():\n",
      "    print(f\"{word}\\t{total}\")\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(f\"cat reducer2.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa7e585-5ec3-4012-87f8-ab441b365cb2",
   "metadata": {},
   "source": [
    "## Run hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2671ba03-8d07-45d7-97b6-e3d28eddbd10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run hadoop for file1.txt with mapper1.py and reducer1.py\n",
    "\n",
    "def runMapReduce(input_dir, output_dir, mapper, reducer, \n",
    "                 hadoop_streamer=\"/usr/local/Cellar/hadoop/3.4.1/libexec/share/hadoop/tools/lib/hadoop-streaming-3.4.1.jar\"):\n",
    "\n",
    "    # Remove old output\n",
    "    remover = ([\"hdfs\", \"dfs\", \"-rm\", \"-r\", output_dir])\n",
    "    subprocess.run(remover, check=True)\n",
    "    \n",
    "    # Run hadoop command in a subprocess\n",
    "    processor = ([\"hadoop\", \"jar\", hadoop_streamer,\n",
    "                        \"-input\", input_dir,\n",
    "                        \"-output\",output_dir, \n",
    "                        \"-mapper\", mapper,\n",
    "                        \"-reducer\", reducer,\n",
    "                        \"-file\", mapper,\n",
    "                        \"-file\", reducer,\n",
    "                        \"-numReduceTasks\", \"1\"\n",
    "                       ])\n",
    "    # Run command\n",
    "    subprocess.run(processor, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0e69d1-8b08-4503-91d3-62f6a460e23e",
   "metadata": {},
   "source": [
    "## File 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4bf0519b-0443-497c-ad7b-6174595b9312",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 23:18:59,075 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/drmariamisayan/output1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 23:19:02,056 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "2025-09-21 23:19:02,435 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [mapper1.py, reducer1.py] [] /var/folders/n_/hr553kvx21n_pmvyjztgw5bh0000gn/T/streamjob18313799321028312200.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 23:19:03,672 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2025-09-21 23:19:03,937 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2025-09-21 23:19:03,937 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
      "2025-09-21 23:19:03,969 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2025-09-21 23:19:05,010 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-09-21 23:19:05,279 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "2025-09-21 23:19:05,691 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local288401792_0001\n",
      "2025-09-21 23:19:05,692 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-09-21 23:19:06,470 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/drmariamisayan/mapreducer_assignment/mapper1.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/job_local288401792_0001_2bfc7dba-e3fd-4e70-936a-05e210316257/mapper1.py\n",
      "2025-09-21 23:19:06,530 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/drmariamisayan/mapreducer_assignment/reducer1.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/job_local288401792_0001_b65c874a-3245-4189-a06d-e3067e0a2dc9/reducer1.py\n",
      "2025-09-21 23:19:06,760 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "2025-09-21 23:19:06,768 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "2025-09-21 23:19:06,768 INFO mapreduce.Job: Running job: job_local288401792_0001\n",
      "2025-09-21 23:19:06,776 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "2025-09-21 23:19:06,871 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2025-09-21 23:19:06,871 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2025-09-21 23:19:06,966 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "2025-09-21 23:19:06,970 INFO mapred.LocalJobRunner: Starting task: attempt_local288401792_0001_m_000000_0\n",
      "2025-09-21 23:19:07,017 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2025-09-21 23:19:07,017 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2025-09-21 23:19:07,033 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "2025-09-21 23:19:07,034 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "2025-09-21 23:19:07,050 INFO mapred.MapTask: Processing split: hdfs://localhost:8020/user/drmariamisayan/input1/file1.txt:0+22335\n",
      "2025-09-21 23:19:07,085 INFO mapred.MapTask: numReduceTasks: 1\n",
      "2025-09-21 23:19:07,131 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2025-09-21 23:19:07,131 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "2025-09-21 23:19:07,131 INFO mapred.MapTask: soft limit at 83886080\n",
      "2025-09-21 23:19:07,131 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "2025-09-21 23:19:07,131 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "2025-09-21 23:19:07,134 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2025-09-21 23:19:07,172 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/drmariamisayan/mapreducer_assignment/./mapper1.py]\n",
      "2025-09-21 23:19:07,184 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "2025-09-21 23:19:07,188 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "2025-09-21 23:19:07,189 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "2025-09-21 23:19:07,189 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "2025-09-21 23:19:07,189 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "2025-09-21 23:19:07,189 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "2025-09-21 23:19:07,190 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "2025-09-21 23:19:07,190 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "2025-09-21 23:19:07,192 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "2025-09-21 23:19:07,193 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "2025-09-21 23:19:07,193 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "2025-09-21 23:19:07,193 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "2025-09-21 23:19:07,360 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2025-09-21 23:19:07,360 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2025-09-21 23:19:07,362 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2025-09-21 23:19:07,383 INFO streaming.PipeMapRed: Records R/W=381/1\n",
      "2025-09-21 23:19:07,391 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "2025-09-21 23:19:07,411 INFO streaming.PipeMapRed: mapRedFinished\n",
      "2025-09-21 23:19:07,416 INFO mapred.LocalJobRunner: \n",
      "2025-09-21 23:19:07,416 INFO mapred.MapTask: Starting flush of map output\n",
      "2025-09-21 23:19:07,416 INFO mapred.MapTask: Spilling map output\n",
      "2025-09-21 23:19:07,416 INFO mapred.MapTask: bufstart = 0; bufend = 28804; bufvoid = 104857600\n",
      "2025-09-21 23:19:07,416 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26199192(104796768); length = 15205/6553600\n",
      "2025-09-21 23:19:07,566 INFO mapred.MapTask: Finished spill 0\n",
      "2025-09-21 23:19:07,617 INFO mapred.Task: Task:attempt_local288401792_0001_m_000000_0 is done. And is in the process of committing\n",
      "2025-09-21 23:19:07,628 INFO mapred.LocalJobRunner: Records R/W=381/1\n",
      "2025-09-21 23:19:07,628 INFO mapred.Task: Task 'attempt_local288401792_0001_m_000000_0' done.\n",
      "2025-09-21 23:19:07,635 INFO mapred.Task: Final Counters for attempt_local288401792_0001_m_000000_0: Counters: 23\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1411\n",
      "\t\tFILE: Number of bytes written=774233\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=22335\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=5\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=1\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=381\n",
      "\t\tMap output records=3802\n",
      "\t\tMap output bytes=28804\n",
      "\t\tMap output materialized bytes=36414\n",
      "\t\tInput split bytes=110\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=3802\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=268435456\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=22335\n",
      "2025-09-21 23:19:07,635 INFO mapred.LocalJobRunner: Finishing task: attempt_local288401792_0001_m_000000_0\n",
      "2025-09-21 23:19:07,635 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "2025-09-21 23:19:07,638 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "2025-09-21 23:19:07,638 INFO mapred.LocalJobRunner: Starting task: attempt_local288401792_0001_r_000000_0\n",
      "2025-09-21 23:19:07,650 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2025-09-21 23:19:07,652 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2025-09-21 23:19:07,653 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "2025-09-21 23:19:07,654 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "2025-09-21 23:19:07,657 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@74d2a037\n",
      "2025-09-21 23:19:07,658 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2025-09-21 23:19:07,712 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=3006477056, maxSingleShuffleLimit=751619264, mergeThreshold=1984274944, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "2025-09-21 23:19:07,714 INFO reduce.EventFetcher: attempt_local288401792_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "2025-09-21 23:19:07,761 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local288401792_0001_m_000000_0 decomp: 36410 len: 36414 to MEMORY\n",
      "2025-09-21 23:19:07,764 INFO reduce.InMemoryMapOutput: Read 36410 bytes from map-output for attempt_local288401792_0001_m_000000_0\n",
      "2025-09-21 23:19:07,766 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 36410, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->36410\n",
      "2025-09-21 23:19:07,768 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "2025-09-21 23:19:07,770 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "2025-09-21 23:19:07,771 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "2025-09-21 23:19:07,806 INFO mapred.Merger: Merging 1 sorted segments\n",
      "2025-09-21 23:19:07,806 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 36403 bytes\n",
      "2025-09-21 23:19:07,869 INFO reduce.MergeManagerImpl: Merged 1 segments, 36410 bytes to disk to satisfy reduce memory limit\n",
      "2025-09-21 23:19:07,870 INFO reduce.MergeManagerImpl: Merging 1 files, 36414 bytes from disk\n",
      "2025-09-21 23:19:07,871 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "2025-09-21 23:19:07,871 INFO mapred.Merger: Merging 1 sorted segments\n",
      "2025-09-21 23:19:07,871 INFO mapreduce.Job: Job job_local288401792_0001 running in uber mode : false\n",
      "2025-09-21 23:19:07,872 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 36403 bytes\n",
      "2025-09-21 23:19:07,873 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-09-21 23:19:07,873 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "2025-09-21 23:19:07,905 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/drmariamisayan/mapreducer_assignment/./reducer1.py]\n",
      "2025-09-21 23:19:07,909 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2025-09-21 23:19:07,912 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "2025-09-21 23:19:07,990 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2025-09-21 23:19:07,991 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2025-09-21 23:19:07,995 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2025-09-21 23:19:08,018 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2025-09-21 23:19:08,049 INFO streaming.PipeMapRed: Records R/W=3802/1\n",
      "2025-09-21 23:19:08,051 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "2025-09-21 23:19:08,070 INFO streaming.PipeMapRed: mapRedFinished\n",
      "2025-09-21 23:19:08,165 INFO mapred.Task: Task:attempt_local288401792_0001_r_000000_0 is done. And is in the process of committing\n",
      "2025-09-21 23:19:08,175 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "2025-09-21 23:19:08,176 INFO mapred.Task: Task attempt_local288401792_0001_r_000000_0 is allowed to commit now\n",
      "2025-09-21 23:19:08,208 INFO output.FileOutputCommitter: Saved output of task 'attempt_local288401792_0001_r_000000_0' to hdfs://localhost:8020/user/drmariamisayan/output1\n",
      "2025-09-21 23:19:08,210 INFO mapred.LocalJobRunner: Records R/W=3802/1 > reduce\n",
      "2025-09-21 23:19:08,210 INFO mapred.Task: Task 'attempt_local288401792_0001_r_000000_0' done.\n",
      "2025-09-21 23:19:08,210 INFO mapred.Task: Final Counters for attempt_local288401792_0001_r_000000_0: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=74271\n",
      "\t\tFILE: Number of bytes written=810647\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=22335\n",
      "\t\tHDFS: Number of bytes written=12105\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1288\n",
      "\t\tReduce shuffle bytes=36414\n",
      "\t\tReduce input records=3802\n",
      "\t\tReduce output records=1288\n",
      "\t\tSpilled Records=3802\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=268435456\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=12105\n",
      "2025-09-21 23:19:08,210 INFO mapred.LocalJobRunner: Finishing task: attempt_local288401792_0001_r_000000_0\n",
      "2025-09-21 23:19:08,211 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "2025-09-21 23:19:08,879 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-09-21 23:19:08,880 INFO mapreduce.Job: Job job_local288401792_0001 completed successfully\n",
      "2025-09-21 23:19:08,895 INFO mapreduce.Job: Counters: 36\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=75682\n",
      "\t\tFILE: Number of bytes written=1584880\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=44670\n",
      "\t\tHDFS: Number of bytes written=12105\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=381\n",
      "\t\tMap output records=3802\n",
      "\t\tMap output bytes=28804\n",
      "\t\tMap output materialized bytes=36414\n",
      "\t\tInput split bytes=110\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1288\n",
      "\t\tReduce shuffle bytes=36414\n",
      "\t\tReduce input records=3802\n",
      "\t\tReduce output records=1288\n",
      "\t\tSpilled Records=7604\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=536870912\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=22335\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=12105\n",
      "2025-09-21 23:19:08,895 INFO streaming.StreamJob: Output directory: /user/drmariamisayan/output1\n"
     ]
    }
   ],
   "source": [
    "runMapReduce(\"/user/drmariamisayan/input1/file1.txt\", \"/user/drmariamisayan/output1\", \"mapper1.py\", \"reducer1.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53871743-2068-4891-8609-79a72ee202d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the output to local dir\n",
    "os.system(f\"rm -r {WD}/output1/part-00000\")\n",
    "os.system(f\"hdfs dfs -get {output1_dir}/part-00000 {WD}/output1/part-00000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2dce6d-52de-4847-963e-3b8bc4d07f7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file1 = pd.read_csv(f\"{WD}/output1/part-00000\", sep='\\t', header=None, names=[\"word\", \"count\"])\n",
    "file1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9448cbaf-08a9-4675-bd92-3559f3afcb13",
   "metadata": {},
   "source": [
    "## File 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "84abb5af-05a6-4ab7-abc4-c7926c1a2747",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 23:29:00,578 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/drmariamisayan/output2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 23:29:03,557 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "2025-09-21 23:29:03,927 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [mapper2.py, reducer2.py] [] /var/folders/n_/hr553kvx21n_pmvyjztgw5bh0000gn/T/streamjob4440137042879722557.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 23:29:05,129 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2025-09-21 23:29:05,401 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2025-09-21 23:29:05,401 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
      "2025-09-21 23:29:05,437 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2025-09-21 23:29:06,373 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-09-21 23:29:06,695 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "2025-09-21 23:29:07,114 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1051865084_0001\n",
      "2025-09-21 23:29:07,114 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-09-21 23:29:07,731 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/drmariamisayan/mapreducer_assignment/mapper2.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/job_local1051865084_0001_6d96a9de-0a39-4571-bf4c-c47d4c33ed76/mapper2.py\n",
      "2025-09-21 23:29:07,789 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/drmariamisayan/mapreducer_assignment/reducer2.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/job_local1051865084_0001_15a1d7a1-d41c-4a5e-94f7-09a88a48cef8/reducer2.py\n",
      "2025-09-21 23:29:08,024 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "2025-09-21 23:29:08,033 INFO mapreduce.Job: Running job: job_local1051865084_0001\n",
      "2025-09-21 23:29:08,033 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "2025-09-21 23:29:08,039 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "2025-09-21 23:29:08,049 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2025-09-21 23:29:08,049 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2025-09-21 23:29:08,138 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "2025-09-21 23:29:08,141 INFO mapred.LocalJobRunner: Starting task: attempt_local1051865084_0001_m_000000_0\n",
      "2025-09-21 23:29:08,187 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2025-09-21 23:29:08,187 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2025-09-21 23:29:08,198 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "2025-09-21 23:29:08,198 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "2025-09-21 23:29:08,216 INFO mapred.MapTask: Processing split: hdfs://localhost:8020/user/drmariamisayan/input2/file2.txt:0+20417\n",
      "2025-09-21 23:29:08,246 INFO mapred.MapTask: numReduceTasks: 1\n",
      "2025-09-21 23:29:08,298 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2025-09-21 23:29:08,298 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "2025-09-21 23:29:08,298 INFO mapred.MapTask: soft limit at 83886080\n",
      "2025-09-21 23:29:08,298 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "2025-09-21 23:29:08,298 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "2025-09-21 23:29:08,301 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2025-09-21 23:29:08,342 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/drmariamisayan/mapreducer_assignment/./mapper2.py]\n",
      "2025-09-21 23:29:08,354 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "2025-09-21 23:29:08,356 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "2025-09-21 23:29:08,358 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "2025-09-21 23:29:08,358 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "2025-09-21 23:29:08,358 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "2025-09-21 23:29:08,358 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "2025-09-21 23:29:08,359 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "2025-09-21 23:29:08,359 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "2025-09-21 23:29:08,362 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "2025-09-21 23:29:08,362 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "2025-09-21 23:29:08,363 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "2025-09-21 23:29:08,363 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "2025-09-21 23:29:08,526 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2025-09-21 23:29:08,526 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2025-09-21 23:29:08,530 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2025-09-21 23:29:08,707 INFO streaming.PipeMapRed: Records R/W=465/1\n",
      "2025-09-21 23:29:08,719 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "2025-09-21 23:29:08,719 INFO streaming.PipeMapRed: mapRedFinished\n",
      "2025-09-21 23:29:08,724 INFO mapred.LocalJobRunner: \n",
      "2025-09-21 23:29:08,725 INFO mapred.MapTask: Starting flush of map output\n",
      "2025-09-21 23:29:08,725 INFO mapred.MapTask: Spilling map output\n",
      "2025-09-21 23:29:08,725 INFO mapred.MapTask: bufstart = 0; bufend = 1614; bufvoid = 104857600\n",
      "2025-09-21 23:29:08,725 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213780(104855120); length = 617/6553600\n",
      "2025-09-21 23:29:08,836 INFO mapred.MapTask: Finished spill 0\n",
      "2025-09-21 23:29:08,885 INFO mapred.Task: Task:attempt_local1051865084_0001_m_000000_0 is done. And is in the process of committing\n",
      "2025-09-21 23:29:08,893 INFO mapred.LocalJobRunner: Records R/W=465/1\n",
      "2025-09-21 23:29:08,893 INFO mapred.Task: Task 'attempt_local1051865084_0001_m_000000_0' done.\n",
      "2025-09-21 23:29:08,904 INFO mapred.Task: Final Counters for attempt_local1051865084_0001_m_000000_0: Counters: 23\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1529\n",
      "\t\tFILE: Number of bytes written=743373\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=20417\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=5\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=1\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=465\n",
      "\t\tMap output records=155\n",
      "\t\tMap output bytes=1614\n",
      "\t\tMap output materialized bytes=1930\n",
      "\t\tInput split bytes=110\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=155\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=268435456\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=20417\n",
      "2025-09-21 23:29:08,904 INFO mapred.LocalJobRunner: Finishing task: attempt_local1051865084_0001_m_000000_0\n",
      "2025-09-21 23:29:08,904 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "2025-09-21 23:29:08,907 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "2025-09-21 23:29:08,908 INFO mapred.LocalJobRunner: Starting task: attempt_local1051865084_0001_r_000000_0\n",
      "2025-09-21 23:29:08,921 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2025-09-21 23:29:08,922 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2025-09-21 23:29:08,923 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "2025-09-21 23:29:08,923 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "2025-09-21 23:29:08,926 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1daffbab\n",
      "2025-09-21 23:29:08,928 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2025-09-21 23:29:08,985 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=3006477056, maxSingleShuffleLimit=751619264, mergeThreshold=1984274944, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "2025-09-21 23:29:08,987 INFO reduce.EventFetcher: attempt_local1051865084_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "2025-09-21 23:29:09,034 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1051865084_0001_m_000000_0 decomp: 1926 len: 1930 to MEMORY\n",
      "2025-09-21 23:29:09,036 INFO reduce.InMemoryMapOutput: Read 1926 bytes from map-output for attempt_local1051865084_0001_m_000000_0\n",
      "2025-09-21 23:29:09,038 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1926, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1926\n",
      "2025-09-21 23:29:09,040 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "2025-09-21 23:29:09,041 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "2025-09-21 23:29:09,042 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "2025-09-21 23:29:09,051 INFO mapreduce.Job: Job job_local1051865084_0001 running in uber mode : false\n",
      "2025-09-21 23:29:09,053 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-09-21 23:29:09,078 INFO mapred.Merger: Merging 1 sorted segments\n",
      "2025-09-21 23:29:09,078 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1917 bytes\n",
      "2025-09-21 23:29:09,111 INFO reduce.MergeManagerImpl: Merged 1 segments, 1926 bytes to disk to satisfy reduce memory limit\n",
      "2025-09-21 23:29:09,112 INFO reduce.MergeManagerImpl: Merging 1 files, 1930 bytes from disk\n",
      "2025-09-21 23:29:09,113 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "2025-09-21 23:29:09,113 INFO mapred.Merger: Merging 1 sorted segments\n",
      "2025-09-21 23:29:09,114 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1917 bytes\n",
      "2025-09-21 23:29:09,114 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "2025-09-21 23:29:09,146 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/drmariamisayan/mapreducer_assignment/./reducer2.py]\n",
      "2025-09-21 23:29:09,150 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2025-09-21 23:29:09,153 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "2025-09-21 23:29:09,226 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2025-09-21 23:29:09,226 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2025-09-21 23:29:09,227 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2025-09-21 23:29:09,284 INFO streaming.PipeMapRed: Records R/W=155/1\n",
      "2025-09-21 23:29:09,286 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "2025-09-21 23:29:09,286 INFO streaming.PipeMapRed: mapRedFinished\n",
      "2025-09-21 23:29:09,794 INFO mapred.Task: Task:attempt_local1051865084_0001_r_000000_0 is done. And is in the process of committing\n",
      "2025-09-21 23:29:09,815 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "2025-09-21 23:29:09,815 INFO mapred.Task: Task attempt_local1051865084_0001_r_000000_0 is allowed to commit now\n",
      "2025-09-21 23:29:09,876 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1051865084_0001_r_000000_0' to hdfs://localhost:8020/user/drmariamisayan/output2\n",
      "2025-09-21 23:29:09,878 INFO mapred.LocalJobRunner: Records R/W=155/1 > reduce\n",
      "2025-09-21 23:29:09,879 INFO mapred.Task: Task 'attempt_local1051865084_0001_r_000000_0' done.\n",
      "2025-09-21 23:29:09,879 INFO mapred.Task: Final Counters for attempt_local1051865084_0001_r_000000_0: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5421\n",
      "\t\tFILE: Number of bytes written=745303\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=20417\n",
      "\t\tHDFS: Number of bytes written=449\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=46\n",
      "\t\tReduce shuffle bytes=1930\n",
      "\t\tReduce input records=155\n",
      "\t\tReduce output records=46\n",
      "\t\tSpilled Records=155\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=268435456\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=449\n",
      "2025-09-21 23:29:09,880 INFO mapred.LocalJobRunner: Finishing task: attempt_local1051865084_0001_r_000000_0\n",
      "2025-09-21 23:29:09,880 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "2025-09-21 23:29:10,060 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-09-21 23:29:10,061 INFO mapreduce.Job: Job job_local1051865084_0001 completed successfully\n",
      "2025-09-21 23:29:10,078 INFO mapreduce.Job: Counters: 36\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=6950\n",
      "\t\tFILE: Number of bytes written=1488676\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=40834\n",
      "\t\tHDFS: Number of bytes written=449\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=465\n",
      "\t\tMap output records=155\n",
      "\t\tMap output bytes=1614\n",
      "\t\tMap output materialized bytes=1930\n",
      "\t\tInput split bytes=110\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=46\n",
      "\t\tReduce shuffle bytes=1930\n",
      "\t\tReduce input records=155\n",
      "\t\tReduce output records=46\n",
      "\t\tSpilled Records=310\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=536870912\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=20417\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=449\n",
      "2025-09-21 23:29:10,078 INFO streaming.StreamJob: Output directory: /user/drmariamisayan/output2\n"
     ]
    }
   ],
   "source": [
    "runMapReduce(\"/user/drmariamisayan/input2/file2.txt\", \"/user/drmariamisayan/output2\", \"mapper2.py\", \"reducer2.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b77b6bb5-7e3f-44c1-ba71-72994c4d667b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 23:29:12,028 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the output to local dir\n",
    "os.system(f\"rm -r {WD}/output2/part-00000\")\n",
    "os.system(f\"hdfs dfs -get {output2_dir}/part-00000 {WD}/output2/part-00000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5da9f3-7bed-4c57-b7a1-450f37068469",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file2 = pd.read_csv(f\"{WD}/output2/part-00000\", sep='\\t', header=None, names=[\"word\", \"count\"])\n",
    "file2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d888bcfe-cbe9-42ec-b956-9e49eb003a41",
   "metadata": {},
   "source": [
    "### Check all contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1a32be25-f73f-4782-87fa-566cd5b59159",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 23:21:36,449 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x   - drmariamisayan supergroup          0 2025-09-21 22:55 input\n",
      "-rw-r--r--   1 drmariamisayan supergroup      22335 2025-09-21 22:55 input/file1.txt\n",
      "-rw-r--r--   1 drmariamisayan supergroup      20417 2025-09-21 22:55 input/file2.txt\n",
      "drwxr-xr-x   - drmariamisayan supergroup          0 2025-09-21 22:59 input1\n",
      "-rw-r--r--   1 drmariamisayan supergroup      22335 2025-09-21 22:59 input1/file1.txt\n",
      "drwxr-xr-x   - drmariamisayan supergroup          0 2025-09-21 22:59 input2\n",
      "-rw-r--r--   1 drmariamisayan supergroup      20417 2025-09-21 22:59 input2/file2.txt\n",
      "drwxr-xr-x   - drmariamisayan supergroup          0 2025-09-21 23:19 output1\n",
      "-rw-r--r--   1 drmariamisayan supergroup          0 2025-09-21 23:19 output1/_SUCCESS\n",
      "-rw-r--r--   1 drmariamisayan supergroup      12105 2025-09-21 23:19 output1/part-00000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check contents\n",
    "os.system(f\"hdfs dfs -ls -R\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312bb6bb-e4a1-45ba-98e0-4fc50eaddb7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9",
   "language": "python",
   "name": "py3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
